{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPToolbox\n",
    "using POMDPModels\n",
    "using Interact\n",
    "value = Interact.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdp = GridWorld()\n",
    "plot(mdp, s->reward(mdp, s, :up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "The cell below runs policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plots = []\n",
    "tol = 1e-5\n",
    "U = zeros(n_states(mdp))\n",
    "\n",
    "while true\n",
    "    Unew = Array{Float64}(n_states(mdp))\n",
    "    \n",
    "    for s in iterator(states(mdp))\n",
    "        Qs = []\n",
    "        for a in iterator(actions(mdp))\n",
    "            \n",
    "            # Calculate expectation of U(s')\n",
    "            exp_Usp = 0.0\n",
    "            tdist = transition(mdp, s, a)\n",
    "            for sp in iterator(tdist)\n",
    "                p = pdf(tdist, sp)\n",
    "                if p > 0.0\n",
    "                    exp_Usp += p*U[state_index(mdp, sp)]\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # Bellman\n",
    "            push!(Qs, reward(mdp, s, a) + discount(mdp)*exp_Usp)\n",
    "        end\n",
    "        \n",
    "        Unew[state_index(mdp, s)] = maximum(Qs)\n",
    "    end\n",
    "    \n",
    "    push!(plots, plot(mdp, U))\n",
    "    \n",
    "    if maximum(abs(U-Unew)) < tol\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    U = Unew\n",
    "end\n",
    "\n",
    "@manipulate for i in 1:length(plots)\n",
    "    plots[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi = Array{Symbol}(n_states(mdp))\n",
    "for s in iterator(states(mdp))\n",
    "    best = -Inf\n",
    "    best_a = nothing\n",
    "\n",
    "    for a in iterator(actions(mdp))\n",
    "        # Calculate expectation of U(s')\n",
    "        exp_Usp = 0.0\n",
    "        tdist = transition(mdp, s, a)\n",
    "        for sp in iterator(tdist)\n",
    "            p = pdf(tdist, sp)\n",
    "            if p > 0.0\n",
    "                exp_Usp += p*U[state_index(mdp, sp)]\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # find the value of taking the action\n",
    "        Q = reward(mdp, s, a) + discount(mdp)*exp_Usp\n",
    "        if Q > best\n",
    "            best = Q\n",
    "            best_a = a\n",
    "        end\n",
    "    end\n",
    "    pi[state_index(mdp, s)] = best_a\n",
    "end\n",
    "\n",
    "plot(mdp, U, VectorPolicy(mdp, pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
